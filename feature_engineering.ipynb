{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "import indicoio\n",
    "import seaborn as sns\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "    good_bag = bag[\"Good Words\"].apply(str.lower).to_list()\n",
    "    good_count = 0\n",
    "    for word in good_bag:\n",
    "        good_count += string_vector.count(word)\n",
    "    good = good_count/len(string_vector)\n",
    "    return good\n",
    "\n",
    "def bad_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "    bad_bag = bag[\"Bad Words\"].apply(str.lower).to_list()\n",
    "    bad_count = 0\n",
    "    for word in bad_bag:\n",
    "        bad_count += string_vector.count(word)\n",
    "    bad = bad_count/len(string_vector)\n",
    "    return bad\n",
    "    \n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "    \n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "    \n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "    \n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(np.mean)\n",
    "    df[\"indico_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "    \n",
    "    # add bag of words featurization\n",
    "    df[\"bad_bag\"] = df[\"vectorized\"].apply(bad_bag)\n",
    "    df[\"good_bag\"] = df[\"vectorized\"].apply(good_bag)\n",
    "    \n",
    "    # return the nice beautiful dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(d, ticky):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    df.lastweek = df.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    X = d[[\"Open\", \"lastweek\", \"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\"]]\n",
    "    Y = d[\"Close\"]\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    def mse(v1, v2):\n",
    "        return np.sum((v1 - v2) ** 2) \n",
    "    \n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = mse(Y_predicted, Y_test)\n",
    "    print(\"aggregated error : {0}\".format(error))\n",
    "    print(\"average error by prediction for {1}: {0}\".format(np.mean(Y_predicted - Y_test), ticky))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ticker(ticky):\n",
    "    print(\"******{0}*********\".format(ticky)*5)\n",
    "    start = time.time()\n",
    "    df = pipeline('data/6m-weekly/{0}-6m-weekly.csv'.format(ticky), ticky)\n",
    "    end = time.time()\n",
    "    pipeline_time = end - start\n",
    "    print(\"pipeline took {0}s for {1}\".format(pipeline_time, ticky))\n",
    "    predictor(df, ticky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['aapl', 'crm', 'tsla', 'fb', 'baba', 'msft', 'nflx', 'amzn', 'googl', 'wmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******aapl***************aapl***************aapl***************aapl***************aapl*********\n",
      "pipeline took 30.648231744766235s for aapl\n",
      "aggregated error : 1.449004747382989e-16\n",
      "average error by prediction for aapl: 3.7402969610411674e-10\n",
      "******crm***************crm***************crm***************crm***************crm*********\n",
      "pipeline took 30.65610694885254s for crm\n",
      "aggregated error : 8.08661352611945e-16\n",
      "average error by prediction for crm: 2.013820221691276e-09\n",
      "******tsla***************tsla***************tsla***************tsla***************tsla*********\n",
      "pipeline took 34.168715953826904s for tsla\n",
      "aggregated error : 2.661535321437689e-16\n",
      "average error by prediction for tsla: -3.6366335128453406e-09\n",
      "******fb***************fb***************fb***************fb***************fb*********\n",
      "pipeline took 30.925320386886597s for fb\n",
      "aggregated error : 1.6780559364645576e-16\n",
      "average error by prediction for fb: 1.6132572808500198e-09\n",
      "******baba***************baba***************baba***************baba***************baba*********\n",
      "pipeline took 33.72789192199707s for baba\n",
      "aggregated error : 4.082547197073833e-16\n",
      "average error by prediction for baba: -4.314102852731594e-09\n",
      "******msft***************msft***************msft***************msft***************msft*********\n",
      "pipeline took 39.31935691833496s for msft\n",
      "aggregated error : 2.1082729627653157e-16\n",
      "average error by prediction for msft: -5.798376100251011e-10\n",
      "******nflx***************nflx***************nflx***************nflx***************nflx*********\n",
      "pipeline took 31.996372938156128s for nflx\n",
      "aggregated error : 7.543924902365253e-17\n",
      "average error by prediction for nflx: -4.0662623885307564e-10\n",
      "******amzn***************amzn***************amzn***************amzn***************amzn*********\n",
      "pipeline took 31.6386981010437s for amzn\n",
      "aggregated error : 1.4351603273334578e-18\n",
      "average error by prediction for amzn: 7.326485097615255e-11\n",
      "******googl***************googl***************googl***************googl***************googl*********\n",
      "pipeline took 36.28045892715454s for googl\n",
      "aggregated error : 5.433963383786518e-17\n",
      "average error by prediction for googl: 1.3722253950416214e-09\n",
      "******wmt***************wmt***************wmt***************wmt***************wmt*********\n",
      "pipeline took 31.908727169036865s for wmt\n",
      "aggregated error : 1.801970593191128e-14\n",
      "average error by prediction for wmt: -1.0006593874499636e-08\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    run_ticker(ticker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
