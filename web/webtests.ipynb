{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a ticker string, YYYY-MM-DD formatted datestring\n",
    "\n",
    "def runner(ticker, date):\n",
    "    return\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "import indicoio\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "# feature functions\n",
    "def good_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "    good_bag = bag[\"Good Words\"].apply(str.lower).to_list()\n",
    "    good_count = 0\n",
    "    for word in good_bag:\n",
    "        good_count += string_vector.count(word)\n",
    "    good = good_count / len(string_vector)\n",
    "    return good\n",
    "\n",
    "\n",
    "def bad_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "    bad_bag = bag[\"Bad Words\"].apply(str.lower).to_list()\n",
    "    bad_count = 0\n",
    "    for word in bad_bag:\n",
    "        bad_count += string_vector.count(word)\n",
    "    bad = bad_count / len(string_vector)\n",
    "    return bad\n",
    "\n",
    "\n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "\n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "\n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "\n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(\n",
    "        np.mean)\n",
    "    df[\"indico_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "\n",
    "    # add bag of words featurization\n",
    "    df[\"bad_bag\"] = df[\"vectorized\"].apply(bad_bag)\n",
    "    df[\"good_bag\"] = df[\"vectorized\"].apply(good_bag)\n",
    "\n",
    "    # change the index so we can access rows by start date\n",
    "    # df.set_index('Start')\n",
    "\n",
    "    # return the nice beautiful dataframe\n",
    "    return df\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return np.sum((v1 - v2) ** 2)\n",
    "\n",
    "\n",
    "def impute(X):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    return X\n",
    "\n",
    "def lm(d, datestring, ticky):\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    d.lastweek = d.lastweek.shift(7)\n",
    "    training_X_cols = [\"Open\", \"lastweek\", \"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\"]\n",
    "    training_Y_col = \"Close\"\n",
    "\n",
    "    # Traning set will be all rows except for the week in question.\n",
    "    X_train = d.loc[d[\"Start\"] != datestring, training_X_cols]\n",
    "    Y_train = d.loc[d[\"Start\"] != datestring, training_Y_col]\n",
    "    X_test = d.loc[d[\"Start\"] == datestring, training_X_cols]\n",
    "    Y_test = d.loc[d[\"Start\"] == datestring, training_Y_col]\n",
    "    \n",
    "    X_train, X_test = impute(X_train), impute(X_test)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = Y_test - Y_predicted\n",
    "    error = error.values\n",
    "    print(\"difference between real and predicted for {0} : {1}\".format(ticky, error))\n",
    "\n",
    "\n",
    "def run_ticker_linear(ticky, datestring):\n",
    "    print(\"******{0}*********\".format(ticky)*3)\n",
    "    start = time.time()\n",
    "    df = pipeline('data/6m-weekly/{0}-6m-weekly.csv'.format(ticky), ticky)\n",
    "    end = time.time()\n",
    "    pipeline_time = end - start\n",
    "    print(\"pipeline took {0}s for {1}\".format(pipeline_time, ticky))\n",
    "    lm(df, datestring, ticky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******nflx***************nflx***************nflx*********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/indicoio/utils/api.py:147: UserWarning: You're out of credits for the month! You're running into your grace credits now.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 79.54975819587708s for nflx\n",
      "difference between real and predicted for nflx : [6.18068668]\n"
     ]
    }
   ],
   "source": [
    "run_ticker_linear(\"nflx\", \"2019-04-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
