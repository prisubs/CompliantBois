{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import indicoio\n",
    "from textblob import TextBlob\n",
    "import _pickle as cPickle\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "[INPUT]\n",
    "ticker: 4-5 digit ticker on nyse [STRING]\n",
    "day: whether prediction is for the next day [BOOLEAN]\n",
    "week: whether prediction is for the next week [BOOLEAN]\n",
    "\n",
    "[OUTPUT]\n",
    "predicted_price: our prediction on tomorrow's price [FLOAT]\n",
    "'''\n",
    "def future_runner(ticker):\n",
    "    today = datetime.date.today()\n",
    "    testing_df = pipeline_linear(ticker, today, dyn=True)\n",
    "    model = create_model_linear(ticker)\n",
    "    prediction = run_model_linear(testing_df, model)\n",
    "    prediction_formatted = \"Our dynamically constructed model predicted ${0} for tomorrow's price.\".format(prediction)\n",
    "    return prediction_formatted\n",
    "\n",
    "'''\n",
    "[INPUT]\n",
    "ticker: 4-5 digit ticker on nyse [STRING]\n",
    "date: yyyy-mm-dd formatted datestring [STRING]\n",
    "\n",
    "[OUTPUT]\n",
    "rating: BUY or SELL [String]\n",
    "delta: \"TICKER went down by AMOUNT.\" [String]\n",
    "good_count: amount of good headlines out of 100 [Int]\n",
    "good_headlines: sampling of five good headlines [List<String>]\n",
    "bad_count: amount of bad headlines out of 100 [Int]\n",
    "bad_headlines: sampling of five bad headlines [List<String>]\n",
    "news_category: GOOD or OKAY or BAD [String]\n",
    "metadata: full company name, sector, industry [List<String>]\n",
    "'''\n",
    "def past_runner(ticker, date):\n",
    "    df = pipeline_logistic(ticker, date)\n",
    "    predicted_delta, actual_delta = run_model_logistic(df, \"data/logistic.pkl\")\n",
    "    rating, delta = translate_delta(predicted_delta), find_delta(df[\"Start\"][0], df[\"End\"][0], ticker)\n",
    "    headlines = df[\"headlines\"][0]\n",
    "    good_headlines, bad_headlines, good_count, bad_count = classify_headlines(headlines)\n",
    "    news_category = make_category(good_count, bad_count)\n",
    "    metadata = make_alias(ticker)\n",
    "    return rating, delta, good_count, good_headlines, bad_count, bad_headlines, news_category, metadata\n",
    "\n",
    "'''\n",
    "**************** UTILITY FUNCTIONS ****************\n",
    "create_model_linear: dynamically constructs linear regression model\n",
    "weekly_visualization: saves a png figure of sentiment by week as weekly_sentiment.png\n",
    "make_alias: turns a ticker into its full name, sector, and industry in a list\n",
    "pretty_print: takes inputs from past_runner, outputs them for testing\n",
    "printlist: pretty prints a list of strings\n",
    "find_delta: outputs a formatted string for stock movement\n",
    "translate_delta: changes label into a buy/sell rating\n",
    "make_category: transforms good/bad headline counts into a status string\n",
    "classify_headlines: returns counts and samples for a single date's headlines\n",
    "single_headlines: processes one headline\n",
    "impute: performs normalization prior to logistic regression\n",
    "good_bag/bad_bag: creates bag of words features from word lists and text corpus\n",
    "aggregate_jsons: turns a response from tiingo client into a corpus\n",
    "pickle_down: unpickles a model to be run on user query\n",
    "six_days: calculates end date of a week from a date\n",
    "remove_time: formats a string in yyyy-mm-dd style\n",
    "pipeline: transforms a single date and row into observation for feature matrix\n",
    "multi_row_pipeline: computes entire feature matrix\n",
    "run_model_logistic: runs logistic regression on test point\n",
    "run_model_linear: runs logistic regression on tomorrow\n",
    "output_graph: saves a weekly sentiment graph to \"week_sent.png\"\n",
    "****************************************************\n",
    "'''\n",
    "\n",
    "\n",
    "def related_tickers(ticker):\n",
    "    industry = make_alias(ticker)[2]\n",
    "    name = make_alias(ticker)[0]\n",
    "    all_ticker_df = pd.read_csv(\"data/ticker_translate.csv\")\n",
    "    related_metadata = all_ticker_df.loc[(all_ticker_df[\"Industry\"] == industry) & (all_ticker_df[\"Name\"] != name), \"Name\"].tolist()[0:5]\n",
    "    return {\n",
    "        \"industry\": industry,\n",
    "        \"related_companies\": related_metadata\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model_linear(ticker):\n",
    "    dates = pd.read_csv(r\"data/dates.csv\")[\"0\"].tolist()\n",
    "    master_df_set = []\n",
    "    for date in dates:\n",
    "        row = pipeline_linear(ticker, date, dyn=False)\n",
    "        master_df_set.append(row)\n",
    "    training_df = pd.concat(master_df_set)\n",
    "    X_train = training_df[[\"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\", \"lastweek\", \"absolute_change\", \"macro_direction\"]]\n",
    "    Y_train = training_df[\"price\"]\n",
    "    model = LinearRegression().fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_alias(ticker):\n",
    "    tickers = pd.read_csv(\"data/ticker_translate.csv\")\n",
    "    ticker = ticker.upper()\n",
    "    data = tickers.loc[tickers[\"Ticker\"] == ticker, [\"Name\", \"Sector\", \"Industry\"]]\n",
    "    return data.values.tolist()[0]  # name, sector, industry\n",
    "\n",
    "\n",
    "def pretty_print(a, b, c, d, e, f, g):\n",
    "    print(\"RATING: {0} \\n{1}\".format(a, b), file=sys.stderr)\n",
    "    print(\"{0} good headlines:\".format(c), file=sys.stderr)\n",
    "    printlist(d)\n",
    "    print(\"{0} bad headlines:\".format(e), file=sys.stderr)\n",
    "    printlist(f)\n",
    "    print(\"news rating: {0}\".format(g), file=sys.stderr)\n",
    "    return\n",
    "\n",
    "\n",
    "def printlist(lis):\n",
    "    for l in lis:\n",
    "        print(\"     \" + str(l), file=sys.stderr)\n",
    "\n",
    "\n",
    "# display functions\n",
    "def find_delta(start, end, ticker):\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    prices = client.get_ticker_price(ticker, fmt='object', startDate=start, endDate=end, frequency='daily')\n",
    "    delta = prices[-1].close - prices[0].open\n",
    "    delta = math.ceil(delta*100)/100\n",
    "    if delta < 0:\n",
    "        return \"{0} went down by {1}.\".format(ticker, delta)\n",
    "    else:\n",
    "        return \"{0} went up by {1}.\".format(ticker, delta)\n",
    "\n",
    "\n",
    "def translate_delta(delta):\n",
    "    if delta == 1:\n",
    "        return \"BUY\"\n",
    "    else:\n",
    "        return \"SELL\"\n",
    "\n",
    "\n",
    "# Headline classifier functions\n",
    "def make_category(goods, bads):\n",
    "    ratio = goods/bads\n",
    "    if ratio > 1.25:\n",
    "        return \"GOOD\" #green\n",
    "    elif ratio > 0.75:\n",
    "        return \"OKAY\" #yellow\n",
    "    else:\n",
    "        return \"BAD\" #red\n",
    "\n",
    "\n",
    "def classify_headlines(headline_list):\n",
    "    bad, good = [], []\n",
    "    for headline in headline_list:\n",
    "        ind_polarity = single_headlines(headline)\n",
    "        if ind_polarity > 0:\n",
    "            good.append([headline, ind_polarity])\n",
    "        else:\n",
    "            bad.append([headline, ind_polarity])\n",
    "    bad_count, good_count = len(bad), len(good)\n",
    "    baddest, goodest = sorted(bad, key=lambda x: x[1], reverse=False), sorted(good, key=lambda x: x[1], reverse=True)\n",
    "    baddest, goodest = [b[0] for b in baddest], [g[0] for g in goodest]\n",
    "    baddest, goodest = baddest[0:5], goodest[0:5]\n",
    "\n",
    "    return goodest, baddest, good_count, bad_count\n",
    "\n",
    "\n",
    "def single_headlines(headline):\n",
    "    headline = TextBlob(headline)\n",
    "    p = headline.sentiment.polarity\n",
    "    return p\n",
    "\n",
    "\n",
    "# feature functions\n",
    "def impute(X):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def good_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/sentiment_word_list.csv\")\n",
    "    good_bag = bag[\"good\"].dropna().apply(str.lower).to_list()\n",
    "    count = 0\n",
    "    ttl_length = 0\n",
    "    string_vector = ' '.join(string_vector)\n",
    "    for word in good_bag:\n",
    "        for word2 in string_vector.split():\n",
    "            ttl_length = ttl_length + 1\n",
    "            if word.lower() == word2.lower():\n",
    "                count = count + 1\n",
    "    return count/ttl_length\n",
    "\n",
    "\n",
    "def bad_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/sentiment_word_list.csv\")\n",
    "    bad_bag = bag[\"bad\"].apply(str.lower).to_list()\n",
    "    count = 0\n",
    "    ttl_length = 0\n",
    "    string_vector = ' '.join(string_vector)\n",
    "    for word in bad_bag:\n",
    "        for word2 in string_vector.split():\n",
    "            ttl_length = ttl_length + 1\n",
    "            if word.lower() == word2.lower():\n",
    "                count = count + 1\n",
    "    return count/ttl_length\n",
    "\n",
    "\n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "\n",
    "def pickle_down(filepath):\n",
    "    with open(filepath, 'rb') as fid:\n",
    "        model_loaded = cPickle.load(fid)\n",
    "    return model_loaded\n",
    "\n",
    "\n",
    "def six_days(start_date):\n",
    "    return start_date + datetime.timedelta(days=6)\n",
    "\n",
    "\n",
    "def remove_time(dt):\n",
    "    return dt[0:10]\n",
    "\n",
    "\n",
    "def featurize_lidar(start, end):\n",
    "    lidars = pd.read_csv(\"data/macros.csv\")\n",
    "    start_lidar = lidars.loc[lidars[\"Date\"] == start, \"Macro\"].tolist()[0]\n",
    "    end_lidar = lidars.loc[lidars[\"Date\"] == end, \"Macro\"].tolist()[0]\n",
    "    absolute_change = end_lidar - start_lidar\n",
    "    if absolute_change < 0:\n",
    "        delt = 0\n",
    "    else:\n",
    "        delt = 1\n",
    "    return absolute_change, delt\n",
    "\n",
    "\n",
    "def base_pipeline(ticker, date, dynamic=False):\n",
    "    # api auth\n",
    "    indicoio.config.api_key = \"04878c9a5bb99aaf8a8ccdd65954442a\"\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "\n",
    "    # fix dates\n",
    "    start_date = pd.to_datetime(pd.Series([date]), infer_datetime_format=True)\n",
    "    end_date = start_date.apply(six_days).apply(str)[0][0:10]\n",
    "    start_date = start_date.apply(str).apply(remove_time)[0]\n",
    "    \n",
    "    # add the macro as two features - up down, and absolute change\n",
    "    absolute_change, macro_direction = featurize_lidar(start_date, end_date)\n",
    "\n",
    "    # add json\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    json = query_ticker(ticker, start_date, end_date)\n",
    "\n",
    "    # create corpus\n",
    "    corpus = aggregate_jsons(json)\n",
    "    vectorized = corpus\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    corpus = combinatric(vectorized)\n",
    "\n",
    "    # add sentiment\n",
    "    mean = lambda listy: sum(listy) / len(listy)\n",
    "    sentiment = (lambda orig: TextBlob(orig).sentiment.polarity)(corpus)\n",
    "    sentiment_test = mean((lambda orig: [TextBlob(o).sentiment.polarity for o in orig])(vectorized))\n",
    "    indico_sentiment = (lambda text: indicoio.sentiment_hq(text))(corpus)\n",
    "\n",
    "    # add bag of words featurization\n",
    "    badbag = bad_bag(vectorized)\n",
    "    goodbag = good_bag(vectorized)\n",
    "\n",
    "    # add last week's data\n",
    "    start_last = pd.to_datetime(pd.Series([start_date])).apply(\n",
    "        lambda start_date: start_date - datetime.timedelta(days=6))\n",
    "    start_last = remove_time(str(start_last.values[0]))\n",
    "    end_last = start_date\n",
    "    lastweek = client.get_ticker_price(ticker, fmt='object', startDate=start_last, endDate=end_last, frequency='daily')\n",
    "    lastweek = lastweek[0].close\n",
    "\n",
    "    # get prices for return\n",
    "    open_price, close_price = 0.0, 0.0\n",
    "    if not dynamic:\n",
    "        prices = client.get_ticker_price(ticker, fmt='object', startDate=start_date, endDate=end_date, frequency='daily')\n",
    "        open_price = prices[0].open\n",
    "        close_price = prices[-1].close\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Start\": pd.Series([start_date]),\n",
    "            \"End\": pd.Series([end_date]),\n",
    "            \"sentiment\": pd.Series([sentiment]),\n",
    "            \"sentiment_test\": pd.Series([sentiment_test]),\n",
    "            \"indico_sentiment\": pd.Series([indico_sentiment]),\n",
    "            \"bad_bag\": pd.Series([badbag]),\n",
    "            \"good_bag\": pd.Series([goodbag]),\n",
    "            \"lastweek\": pd.Series([lastweek]),\n",
    "            \"headlines\": pd.Series([vectorized]),\n",
    "            \"absolute_change\": pd.Series([absolute_change]),\n",
    "            \"macro_direction\": pd.Series([macro_direction])\n",
    "        }\n",
    "    )\n",
    "    return df, open_price, close_price\n",
    "\n",
    "\n",
    "def pipeline_linear(ticker, date, dyn):\n",
    "    df, open_price, close_price = base_pipeline(ticker, date, dynamic=dyn)\n",
    "    df[\"price\"] = pd.Series([close_price])\n",
    "    return df\n",
    "\n",
    "\n",
    "def pipeline_logistic(ticker, date):\n",
    "    df, open_price, close_price = base_pipeline(ticker, date)\n",
    "    \n",
    "    # add the delta - up or down\n",
    "    tri_delt = close_price - open_price\n",
    "    if tri_delt > 0:\n",
    "        delta = 1\n",
    "    else:\n",
    "        delta = 0\n",
    "\n",
    "    df[\"delta\"] = pd.Series([delta])\n",
    "    return df\n",
    "\n",
    "\n",
    "def multi_row_pipeline(dates, ticker, pipeline_function=pipeline_logistic):\n",
    "    rows = []\n",
    "    for date in dates:\n",
    "        row = pipeline_function(ticker, date)\n",
    "        rows.append(row)\n",
    "        print(\"finished {0} for {1}\".format(date, ticker))\n",
    "    df = pd.concat(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_model_linear(df, model):\n",
    "    X_test = df[[\"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\", \"lastweek\", \"absolute_change\", \"macro_direction\"]]\n",
    "    prediction = model.predict(X_test).tolist()[0]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# returns in the form PREDICTED, ACTUAL =====> two values need to be unpacked\n",
    "def run_model_logistic(df, model_path):\n",
    "    model = pickle_down(model_path)\n",
    "\n",
    "    X_test = df[[\"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\", \"absolute_change\", \"macro_direction\"]]\n",
    "    Y_test = df[\"delta\"]\n",
    "\n",
    "    X_test = impute(X_test)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "\n",
    "    return Y_predicted.tolist(), Y_test.tolist()\n",
    "\n",
    "\n",
    "def output_graph(ticker, start_date):\n",
    "    docs, dates, prices = weekly_data_gather(ticker, start_date)\n",
    "    sentiments = weekly_doc_aggregator(docs)\n",
    "    df = fix_visualize(dates, prices, sentiments)\n",
    "    perform_visualization(df, ticker)\n",
    "    return\n",
    "\n",
    "\n",
    "def fix_visualize(dates, prices, sentiments):\n",
    "    df = pd.DataFrame({\"dates\": dates, \"prices\": prices, \"sent\": sentiments})\n",
    "    df = df.replace(0, pd.np.nan).dropna(axis=0, how='any')\n",
    "    return df\n",
    "\n",
    "\n",
    "def perform_visualization(df, ticker):\n",
    "    dates, sentiments, prices = df[\"dates\"], df[\"sent\"], df[\"prices\"]\n",
    "    fig = sns.lineplot(dates, sentiments, color='black')\n",
    "    ax2 = plt.twinx()\n",
    "    sns.lineplot(dates, prices, ax=ax2, color='green')\n",
    "    fig.set_title(\"sentiment of {0} from {1} to {2}\".format(ticker, dates[0], dates[len(dates)-1]))\n",
    "    fig.set_xticklabels(dates)\n",
    "    for item in fig.get_xticklabels():\n",
    "        item.set_rotation(60)\n",
    "    plt.savefig(\"web/testing.png\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def weekly_data_gather(ticker, start_date):\n",
    "    r, dates, prices = [], [], []\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    dt_obj = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=d1, endDate=d2)\n",
    "    get_price = lambda t, s, e: client.get_ticker_price(ticker, fmt='object',\n",
    "                                                        startDate=s, endDate=e, frequency='daily')\n",
    "    for i in range(7):\n",
    "        d1 = dt_obj + datetime.timedelta(days=i)\n",
    "        d2 = dt_obj + datetime.timedelta(days=i + 1)\n",
    "        d1, d2 = str(d1.strftime(\"%Y-%m-%d\")), str(d2.strftime(\"%Y-%m-%d\"))\n",
    "        json = query_ticker(ticker, d1, d2)\n",
    "        price = get_price(ticker, d1, d2)\n",
    "        if not price:\n",
    "            price = 0\n",
    "        else:\n",
    "            price = price[0].open\n",
    "        docs = [j[\"description\"] for j in json]\n",
    "        r.append(docs)\n",
    "        dates.append(d1)\n",
    "        prices.append(price)\n",
    "    return r, dates, prices\n",
    "\n",
    "\n",
    "def weekly_doc_aggregator(documents_by_day):\n",
    "    sentiments = []\n",
    "    for day in documents_by_day:\n",
    "        total_sentiment = 0\n",
    "        for article in day:\n",
    "            total_sentiment += TextBlob(article).sentiment.polarity\n",
    "        sentiments.append(total_sentiment)\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>2.505630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>2.514005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>2.518193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>2.522380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>2.519880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Macro\n",
       "0  2018-12-24  2.505630\n",
       "1  2018-12-25  2.514005\n",
       "2  2018-12-26  2.518193\n",
       "3  2018-12-27  2.522380\n",
       "4  2018-12-28  2.519880"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macros = pd.read_csv(\"data/macros.csv\")[[\"Date\", \"Macro\"]]\n",
    "macros.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_test</th>\n",
       "      <th>indico_sentiment</th>\n",
       "      <th>bad_bag</th>\n",
       "      <th>good_bag</th>\n",
       "      <th>lastweek</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>2018-12-30 00:00:00</td>\n",
       "      <td>0.115305</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.459462</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>166.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "      <td>0.115305</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.459462</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>157.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>2019-01-13 00:00:00</td>\n",
       "      <td>0.115305</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.459462</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>157.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>2019-01-20 00:00:00</td>\n",
       "      <td>0.115305</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.459462</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>150.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>2019-01-27 00:00:00</td>\n",
       "      <td>0.115305</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.459462</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>153.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Start                  End  sentiment  sentiment_test  \\\n",
       "0           0  2018-12-24  2018-12-30 00:00:00   0.115305          0.0958   \n",
       "1           0  2018-12-31  2019-01-06 00:00:00   0.115305          0.0958   \n",
       "2           0  2019-01-07  2019-01-13 00:00:00   0.115305          0.0958   \n",
       "3           0  2019-01-14  2019-01-20 00:00:00   0.115305          0.0958   \n",
       "4           0  2019-01-21  2019-01-27 00:00:00   0.115305          0.0958   \n",
       "\n",
       "   indico_sentiment   bad_bag  good_bag  lastweek  delta  \n",
       "0          0.459462  0.000005  0.000023    166.07      1  \n",
       "1          0.459462  0.000005  0.000023    157.17      0  \n",
       "2          0.459462  0.000005  0.000023    157.92      1  \n",
       "3          0.459462  0.000005  0.000023    150.75      1  \n",
       "4          0.459462  0.000005  0.000023    153.07      1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_forty = pd.read_csv(\"data/240_rows.csv\")\n",
    "two_forty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-12-24', '2018-12-31', '2019-01-07']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.read_csv(\"data/dates.csv\")\n",
    "dates.columns = [\"Start\"]\n",
    "dates = dates[\"Start\"].tolist()\n",
    "dates[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running data through logistic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"aapl\", \"amzn\", \"fb\", \"crm\", \"msft\", \"baba\", \"wmt\", \"nflx\", \"tsla\", \"googl\", \"akam\", \"acn\", \n",
    "           \"adsk\", \"bac\", \"bby\", \"blk\", \"ebay\", \"hpe\", \"intc\", \"pypl\", \"snps\", \"tmus\", \"trip\", \"dis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 2018-12-24 for aapl\n",
      "finished 2018-12-31 for aapl\n",
      "finished 2019-01-07 for aapl\n",
      "finished 2019-01-14 for aapl\n",
      "finished 2019-01-21 for aapl\n",
      "finished 2019-01-28 for aapl\n",
      "finished 2019-02-04 for aapl\n",
      "finished 2019-02-11 for aapl\n",
      "finished 2019-02-18 for aapl\n",
      "finished 2019-02-25 for aapl\n",
      "finished 2019-03-04 for aapl\n",
      "finished 2019-03-11 for aapl\n",
      "finished 2019-03-18 for aapl\n",
      "finished 2019-03-25 for aapl\n",
      "finished 2019-04-01 for aapl\n",
      "finished 2019-04-08 for aapl\n",
      "finished 2019-04-15 for aapl\n",
      "finished 2019-04-22 for aapl\n",
      "finished 2019-04-29 for aapl\n",
      "finished 2019-05-06 for aapl\n",
      "finished 2019-05-13 for aapl\n",
      "finished 2019-05-20 for aapl\n",
      "finished 2019-05-27 for aapl\n",
      "finished 2019-06-03 for aapl\n",
      "finished 2019-06-10 for aapl\n",
      "finished 2019-06-17 for aapl\n",
      "finished 2019-06-24 for aapl\n",
      "************FINISHED WITH aapl************\n",
      "finished 2018-12-24 for amzn\n",
      "finished 2018-12-31 for amzn\n",
      "finished 2019-01-07 for amzn\n",
      "finished 2019-01-14 for amzn\n",
      "finished 2019-01-21 for amzn\n",
      "finished 2019-01-28 for amzn\n",
      "finished 2019-02-04 for amzn\n",
      "finished 2019-02-11 for amzn\n",
      "finished 2019-02-18 for amzn\n",
      "finished 2019-02-25 for amzn\n",
      "finished 2019-03-04 for amzn\n",
      "finished 2019-03-11 for amzn\n",
      "finished 2019-03-18 for amzn\n",
      "finished 2019-03-25 for amzn\n",
      "finished 2019-04-01 for amzn\n",
      "finished 2019-04-08 for amzn\n",
      "finished 2019-04-15 for amzn\n",
      "finished 2019-04-22 for amzn\n",
      "finished 2019-04-29 for amzn\n",
      "finished 2019-05-06 for amzn\n",
      "finished 2019-05-13 for amzn\n",
      "finished 2019-05-20 for amzn\n",
      "finished 2019-05-27 for amzn\n",
      "finished 2019-06-03 for amzn\n",
      "finished 2019-06-10 for amzn\n",
      "finished 2019-06-17 for amzn\n",
      "finished 2019-06-24 for amzn\n",
      "************FINISHED WITH amzn************\n",
      "finished 2018-12-24 for fb\n",
      "finished 2018-12-31 for fb\n",
      "finished 2019-01-07 for fb\n",
      "finished 2019-01-14 for fb\n",
      "finished 2019-01-21 for fb\n",
      "finished 2019-01-28 for fb\n",
      "finished 2019-02-04 for fb\n",
      "finished 2019-02-11 for fb\n",
      "finished 2019-02-18 for fb\n",
      "finished 2019-02-25 for fb\n",
      "finished 2019-03-04 for fb\n",
      "finished 2019-03-11 for fb\n",
      "finished 2019-03-18 for fb\n",
      "finished 2019-03-25 for fb\n",
      "finished 2019-04-01 for fb\n",
      "finished 2019-04-08 for fb\n",
      "finished 2019-04-15 for fb\n",
      "finished 2019-04-22 for fb\n",
      "finished 2019-04-29 for fb\n",
      "finished 2019-05-06 for fb\n",
      "finished 2019-05-13 for fb\n",
      "finished 2019-05-20 for fb\n",
      "finished 2019-05-27 for fb\n",
      "finished 2019-06-03 for fb\n",
      "finished 2019-06-10 for fb\n",
      "finished 2019-06-17 for fb\n",
      "finished 2019-06-24 for fb\n",
      "************FINISHED WITH fb************\n",
      "finished 2018-12-24 for crm\n",
      "finished 2018-12-31 for crm\n",
      "finished 2019-01-07 for crm\n",
      "finished 2019-01-14 for crm\n",
      "finished 2019-01-21 for crm\n",
      "finished 2019-01-28 for crm\n",
      "finished 2019-02-04 for crm\n",
      "finished 2019-02-11 for crm\n",
      "finished 2019-02-18 for crm\n",
      "finished 2019-02-25 for crm\n",
      "finished 2019-03-04 for crm\n",
      "finished 2019-03-11 for crm\n",
      "finished 2019-03-18 for crm\n",
      "finished 2019-03-25 for crm\n",
      "finished 2019-04-01 for crm\n",
      "finished 2019-04-08 for crm\n",
      "finished 2019-04-15 for crm\n",
      "finished 2019-04-22 for crm\n",
      "finished 2019-04-29 for crm\n",
      "finished 2019-05-06 for crm\n",
      "finished 2019-05-13 for crm\n",
      "finished 2019-05-20 for crm\n",
      "finished 2019-05-27 for crm\n",
      "finished 2019-06-03 for crm\n",
      "finished 2019-06-10 for crm\n",
      "finished 2019-06-17 for crm\n",
      "finished 2019-06-24 for crm\n",
      "************FINISHED WITH crm************\n",
      "finished 2018-12-24 for msft\n",
      "finished 2018-12-31 for msft\n",
      "finished 2019-01-07 for msft\n",
      "finished 2019-01-14 for msft\n",
      "finished 2019-01-21 for msft\n",
      "finished 2019-01-28 for msft\n",
      "finished 2019-02-04 for msft\n",
      "finished 2019-02-11 for msft\n",
      "finished 2019-02-18 for msft\n",
      "finished 2019-02-25 for msft\n",
      "finished 2019-03-04 for msft\n",
      "finished 2019-03-11 for msft\n",
      "finished 2019-03-18 for msft\n",
      "finished 2019-03-25 for msft\n",
      "finished 2019-04-01 for msft\n",
      "finished 2019-04-08 for msft\n",
      "finished 2019-04-15 for msft\n",
      "finished 2019-04-22 for msft\n",
      "finished 2019-04-29 for msft\n",
      "finished 2019-05-06 for msft\n",
      "finished 2019-05-13 for msft\n",
      "finished 2019-05-20 for msft\n",
      "finished 2019-05-27 for msft\n",
      "finished 2019-06-03 for msft\n",
      "finished 2019-06-10 for msft\n",
      "finished 2019-06-17 for msft\n",
      "finished 2019-06-24 for msft\n",
      "************FINISHED WITH msft************\n",
      "finished 2018-12-24 for baba\n",
      "finished 2018-12-31 for baba\n",
      "finished 2019-01-07 for baba\n",
      "finished 2019-01-14 for baba\n",
      "finished 2019-01-21 for baba\n",
      "finished 2019-01-28 for baba\n",
      "finished 2019-02-04 for baba\n",
      "finished 2019-02-11 for baba\n",
      "finished 2019-02-18 for baba\n",
      "finished 2019-02-25 for baba\n",
      "finished 2019-03-04 for baba\n",
      "finished 2019-03-11 for baba\n",
      "finished 2019-03-18 for baba\n",
      "finished 2019-03-25 for baba\n",
      "finished 2019-04-01 for baba\n",
      "finished 2019-04-08 for baba\n",
      "finished 2019-04-15 for baba\n",
      "finished 2019-04-22 for baba\n",
      "finished 2019-04-29 for baba\n",
      "finished 2019-05-06 for baba\n",
      "finished 2019-05-13 for baba\n",
      "finished 2019-05-20 for baba\n",
      "finished 2019-05-27 for baba\n",
      "finished 2019-06-03 for baba\n",
      "finished 2019-06-10 for baba\n",
      "finished 2019-06-17 for baba\n",
      "finished 2019-06-24 for baba\n",
      "************FINISHED WITH baba************\n",
      "finished 2018-12-24 for wmt\n",
      "finished 2018-12-31 for wmt\n",
      "finished 2019-01-07 for wmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/indicoio/utils/api.py:147: UserWarning: You're out of credits for the month! You're running into your grace credits now.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 2019-01-14 for wmt\n",
      "finished 2019-01-21 for wmt\n",
      "finished 2019-01-28 for wmt\n",
      "finished 2019-02-04 for wmt\n",
      "finished 2019-02-11 for wmt\n",
      "finished 2019-02-18 for wmt\n",
      "finished 2019-02-25 for wmt\n",
      "finished 2019-03-04 for wmt\n",
      "finished 2019-03-11 for wmt\n",
      "finished 2019-03-18 for wmt\n",
      "finished 2019-03-25 for wmt\n",
      "finished 2019-04-01 for wmt\n",
      "finished 2019-04-08 for wmt\n",
      "finished 2019-04-15 for wmt\n",
      "finished 2019-04-22 for wmt\n",
      "finished 2019-04-29 for wmt\n",
      "finished 2019-05-06 for wmt\n",
      "finished 2019-05-13 for wmt\n",
      "finished 2019-05-20 for wmt\n",
      "finished 2019-05-27 for wmt\n",
      "finished 2019-06-03 for wmt\n",
      "finished 2019-06-10 for wmt\n",
      "finished 2019-06-17 for wmt\n",
      "finished 2019-06-24 for wmt\n",
      "************FINISHED WITH wmt************\n",
      "finished 2018-12-24 for nflx\n",
      "finished 2018-12-31 for nflx\n",
      "finished 2019-01-07 for nflx\n",
      "finished 2019-01-14 for nflx\n",
      "finished 2019-01-21 for nflx\n",
      "finished 2019-01-28 for nflx\n",
      "finished 2019-02-04 for nflx\n",
      "finished 2019-02-11 for nflx\n",
      "finished 2019-02-18 for nflx\n",
      "finished 2019-02-25 for nflx\n",
      "finished 2019-03-04 for nflx\n",
      "finished 2019-03-11 for nflx\n",
      "finished 2019-03-18 for nflx\n",
      "finished 2019-03-25 for nflx\n",
      "finished 2019-04-01 for nflx\n",
      "finished 2019-04-08 for nflx\n",
      "finished 2019-04-15 for nflx\n",
      "finished 2019-04-22 for nflx\n",
      "finished 2019-04-29 for nflx\n",
      "finished 2019-05-06 for nflx\n",
      "finished 2019-05-13 for nflx\n",
      "finished 2019-05-20 for nflx\n",
      "finished 2019-05-27 for nflx\n",
      "finished 2019-06-03 for nflx\n",
      "finished 2019-06-10 for nflx\n",
      "finished 2019-06-17 for nflx\n",
      "finished 2019-06-24 for nflx\n",
      "************FINISHED WITH nflx************\n",
      "finished 2018-12-24 for tsla\n",
      "finished 2018-12-31 for tsla\n",
      "finished 2019-01-07 for tsla\n",
      "finished 2019-01-14 for tsla\n",
      "finished 2019-01-21 for tsla\n",
      "finished 2019-01-28 for tsla\n",
      "finished 2019-02-04 for tsla\n",
      "finished 2019-02-11 for tsla\n",
      "finished 2019-02-18 for tsla\n",
      "finished 2019-02-25 for tsla\n",
      "finished 2019-03-04 for tsla\n",
      "finished 2019-03-11 for tsla\n",
      "finished 2019-03-18 for tsla\n",
      "finished 2019-03-25 for tsla\n",
      "finished 2019-04-01 for tsla\n",
      "finished 2019-04-08 for tsla\n",
      "finished 2019-04-15 for tsla\n",
      "finished 2019-04-22 for tsla\n",
      "finished 2019-04-29 for tsla\n",
      "finished 2019-05-06 for tsla\n",
      "finished 2019-05-13 for tsla\n",
      "finished 2019-05-20 for tsla\n",
      "finished 2019-05-27 for tsla\n",
      "finished 2019-06-03 for tsla\n",
      "finished 2019-06-10 for tsla\n",
      "finished 2019-06-17 for tsla\n",
      "finished 2019-06-24 for tsla\n",
      "************FINISHED WITH tsla************\n",
      "finished 2018-12-24 for googl\n",
      "finished 2018-12-31 for googl\n",
      "finished 2019-01-07 for googl\n",
      "finished 2019-01-14 for googl\n",
      "finished 2019-01-21 for googl\n",
      "finished 2019-01-28 for googl\n",
      "finished 2019-02-04 for googl\n",
      "finished 2019-02-11 for googl\n",
      "finished 2019-02-18 for googl\n",
      "finished 2019-02-25 for googl\n",
      "finished 2019-03-04 for googl\n",
      "finished 2019-03-11 for googl\n",
      "finished 2019-03-18 for googl\n",
      "finished 2019-03-25 for googl\n",
      "finished 2019-04-01 for googl\n",
      "finished 2019-04-08 for googl\n",
      "finished 2019-04-15 for googl\n",
      "finished 2019-04-22 for googl\n",
      "finished 2019-04-29 for googl\n",
      "finished 2019-05-06 for googl\n",
      "finished 2019-05-13 for googl\n",
      "finished 2019-05-20 for googl\n",
      "finished 2019-05-27 for googl\n",
      "finished 2019-06-03 for googl\n",
      "finished 2019-06-10 for googl\n",
      "finished 2019-06-17 for googl\n",
      "finished 2019-06-24 for googl\n",
      "************FINISHED WITH googl************\n",
      "finished 2018-12-24 for akam\n",
      "finished 2018-12-31 for akam\n",
      "finished 2019-01-07 for akam\n",
      "finished 2019-01-14 for akam\n",
      "finished 2019-01-21 for akam\n",
      "finished 2019-01-28 for akam\n",
      "finished 2019-02-04 for akam\n",
      "finished 2019-02-11 for akam\n",
      "finished 2019-02-18 for akam\n",
      "finished 2019-02-25 for akam\n",
      "finished 2019-03-04 for akam\n",
      "finished 2019-03-11 for akam\n",
      "finished 2019-03-18 for akam\n",
      "finished 2019-03-25 for akam\n",
      "finished 2019-04-01 for akam\n",
      "finished 2019-04-08 for akam\n",
      "finished 2019-04-15 for akam\n",
      "finished 2019-04-22 for akam\n",
      "finished 2019-04-29 for akam\n",
      "finished 2019-05-06 for akam\n",
      "finished 2019-05-13 for akam\n",
      "finished 2019-05-20 for akam\n",
      "finished 2019-05-27 for akam\n",
      "finished 2019-06-03 for akam\n",
      "finished 2019-06-10 for akam\n",
      "finished 2019-06-17 for akam\n",
      "finished 2019-06-24 for akam\n",
      "************FINISHED WITH akam************\n",
      "finished 2018-12-24 for acn\n",
      "finished 2018-12-31 for acn\n",
      "finished 2019-01-07 for acn\n",
      "finished 2019-01-14 for acn\n",
      "finished 2019-01-21 for acn\n",
      "finished 2019-01-28 for acn\n",
      "finished 2019-02-04 for acn\n",
      "finished 2019-02-11 for acn\n",
      "finished 2019-02-18 for acn\n",
      "finished 2019-02-25 for acn\n",
      "finished 2019-03-04 for acn\n",
      "finished 2019-03-11 for acn\n",
      "finished 2019-03-18 for acn\n",
      "finished 2019-03-25 for acn\n",
      "finished 2019-04-01 for acn\n",
      "finished 2019-04-08 for acn\n",
      "finished 2019-04-15 for acn\n",
      "finished 2019-04-22 for acn\n",
      "finished 2019-04-29 for acn\n",
      "finished 2019-05-06 for acn\n",
      "finished 2019-05-13 for acn\n",
      "finished 2019-05-20 for acn\n",
      "finished 2019-05-27 for acn\n",
      "finished 2019-06-03 for acn\n",
      "finished 2019-06-10 for acn\n",
      "finished 2019-06-17 for acn\n",
      "finished 2019-06-24 for acn\n",
      "************FINISHED WITH acn************\n",
      "finished 2018-12-24 for adsk\n",
      "finished 2018-12-31 for adsk\n",
      "finished 2019-01-07 for adsk\n",
      "finished 2019-01-14 for adsk\n",
      "finished 2019-01-21 for adsk\n",
      "finished 2019-01-28 for adsk\n",
      "finished 2019-02-04 for adsk\n",
      "finished 2019-02-11 for adsk\n",
      "finished 2019-02-18 for adsk\n",
      "finished 2019-02-25 for adsk\n",
      "finished 2019-03-04 for adsk\n",
      "finished 2019-03-11 for adsk\n",
      "finished 2019-03-18 for adsk\n",
      "finished 2019-03-25 for adsk\n",
      "finished 2019-04-01 for adsk\n",
      "finished 2019-04-08 for adsk\n",
      "finished 2019-04-15 for adsk\n",
      "finished 2019-04-22 for adsk\n",
      "finished 2019-04-29 for adsk\n",
      "finished 2019-05-06 for adsk\n",
      "finished 2019-05-13 for adsk\n",
      "finished 2019-05-20 for adsk\n",
      "finished 2019-05-27 for adsk\n",
      "finished 2019-06-03 for adsk\n",
      "finished 2019-06-10 for adsk\n",
      "finished 2019-06-17 for adsk\n",
      "finished 2019-06-24 for adsk\n",
      "************FINISHED WITH adsk************\n",
      "finished 2018-12-24 for bac\n",
      "finished 2018-12-31 for bac\n",
      "finished 2019-01-07 for bac\n",
      "finished 2019-01-14 for bac\n",
      "finished 2019-01-21 for bac\n",
      "finished 2019-01-28 for bac\n",
      "finished 2019-02-04 for bac\n",
      "finished 2019-02-11 for bac\n",
      "finished 2019-02-18 for bac\n",
      "finished 2019-02-25 for bac\n",
      "finished 2019-03-04 for bac\n",
      "finished 2019-03-11 for bac\n",
      "finished 2019-03-18 for bac\n",
      "finished 2019-03-25 for bac\n",
      "finished 2019-04-01 for bac\n",
      "finished 2019-04-08 for bac\n",
      "finished 2019-04-15 for bac\n",
      "finished 2019-04-22 for bac\n",
      "finished 2019-04-29 for bac\n",
      "finished 2019-05-06 for bac\n",
      "finished 2019-05-13 for bac\n",
      "finished 2019-05-20 for bac\n",
      "finished 2019-05-27 for bac\n",
      "finished 2019-06-03 for bac\n",
      "finished 2019-06-10 for bac\n",
      "finished 2019-06-17 for bac\n",
      "finished 2019-06-24 for bac\n",
      "************FINISHED WITH bac************\n",
      "finished 2018-12-24 for bby\n",
      "finished 2018-12-31 for bby\n",
      "finished 2019-01-07 for bby\n",
      "finished 2019-01-14 for bby\n",
      "finished 2019-01-21 for bby\n",
      "finished 2019-01-28 for bby\n",
      "finished 2019-02-04 for bby\n",
      "finished 2019-02-11 for bby\n",
      "finished 2019-02-18 for bby\n",
      "finished 2019-02-25 for bby\n",
      "finished 2019-03-04 for bby\n",
      "finished 2019-03-11 for bby\n",
      "finished 2019-03-18 for bby\n",
      "finished 2019-03-25 for bby\n",
      "finished 2019-04-01 for bby\n",
      "finished 2019-04-08 for bby\n",
      "finished 2019-04-15 for bby\n",
      "finished 2019-04-22 for bby\n",
      "finished 2019-04-29 for bby\n",
      "finished 2019-05-06 for bby\n",
      "finished 2019-05-13 for bby\n",
      "finished 2019-05-20 for bby\n",
      "finished 2019-05-27 for bby\n",
      "finished 2019-06-03 for bby\n",
      "finished 2019-06-10 for bby\n",
      "finished 2019-06-17 for bby\n",
      "finished 2019-06-24 for bby\n",
      "************FINISHED WITH bby************\n",
      "finished 2018-12-24 for blk\n",
      "finished 2018-12-31 for blk\n",
      "finished 2019-01-07 for blk\n",
      "finished 2019-01-14 for blk\n",
      "finished 2019-01-21 for blk\n",
      "finished 2019-01-28 for blk\n",
      "finished 2019-02-04 for blk\n",
      "finished 2019-02-11 for blk\n",
      "finished 2019-02-18 for blk\n",
      "finished 2019-02-25 for blk\n",
      "finished 2019-03-04 for blk\n",
      "finished 2019-03-11 for blk\n",
      "finished 2019-03-18 for blk\n",
      "finished 2019-03-25 for blk\n",
      "finished 2019-04-01 for blk\n",
      "finished 2019-04-08 for blk\n",
      "finished 2019-04-15 for blk\n",
      "finished 2019-04-22 for blk\n",
      "finished 2019-04-29 for blk\n",
      "finished 2019-05-06 for blk\n",
      "finished 2019-05-13 for blk\n",
      "finished 2019-05-20 for blk\n",
      "finished 2019-05-27 for blk\n",
      "finished 2019-06-03 for blk\n",
      "finished 2019-06-10 for blk\n",
      "finished 2019-06-17 for blk\n",
      "finished 2019-06-24 for blk\n",
      "************FINISHED WITH blk************\n",
      "finished 2018-12-24 for ebay\n",
      "finished 2018-12-31 for ebay\n",
      "finished 2019-01-07 for ebay\n",
      "finished 2019-01-14 for ebay\n",
      "finished 2019-01-21 for ebay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 2019-01-28 for ebay\n",
      "finished 2019-02-04 for ebay\n",
      "finished 2019-02-11 for ebay\n",
      "finished 2019-02-18 for ebay\n",
      "finished 2019-02-25 for ebay\n",
      "finished 2019-03-04 for ebay\n",
      "finished 2019-03-11 for ebay\n",
      "finished 2019-03-18 for ebay\n",
      "finished 2019-03-25 for ebay\n",
      "finished 2019-04-01 for ebay\n",
      "finished 2019-04-08 for ebay\n",
      "finished 2019-04-15 for ebay\n",
      "finished 2019-04-22 for ebay\n",
      "finished 2019-04-29 for ebay\n",
      "finished 2019-05-06 for ebay\n",
      "finished 2019-05-13 for ebay\n",
      "finished 2019-05-20 for ebay\n",
      "finished 2019-05-27 for ebay\n",
      "finished 2019-06-03 for ebay\n",
      "finished 2019-06-10 for ebay\n",
      "finished 2019-06-17 for ebay\n",
      "finished 2019-06-24 for ebay\n",
      "************FINISHED WITH ebay************\n",
      "finished 2018-12-24 for hpe\n",
      "finished 2018-12-31 for hpe\n",
      "finished 2019-01-07 for hpe\n",
      "finished 2019-01-14 for hpe\n",
      "finished 2019-01-21 for hpe\n",
      "finished 2019-01-28 for hpe\n",
      "finished 2019-02-04 for hpe\n",
      "finished 2019-02-11 for hpe\n",
      "finished 2019-02-18 for hpe\n",
      "finished 2019-02-25 for hpe\n",
      "finished 2019-03-04 for hpe\n",
      "finished 2019-03-11 for hpe\n",
      "finished 2019-03-18 for hpe\n",
      "finished 2019-03-25 for hpe\n",
      "finished 2019-04-01 for hpe\n",
      "finished 2019-04-08 for hpe\n",
      "finished 2019-04-15 for hpe\n",
      "finished 2019-04-22 for hpe\n",
      "finished 2019-04-29 for hpe\n",
      "finished 2019-05-06 for hpe\n",
      "finished 2019-05-13 for hpe\n",
      "finished 2019-05-20 for hpe\n",
      "finished 2019-05-27 for hpe\n",
      "finished 2019-06-03 for hpe\n",
      "finished 2019-06-10 for hpe\n",
      "finished 2019-06-17 for hpe\n",
      "finished 2019-06-24 for hpe\n",
      "************FINISHED WITH hpe************\n",
      "finished 2018-12-24 for intc\n",
      "finished 2018-12-31 for intc\n",
      "finished 2019-01-07 for intc\n",
      "finished 2019-01-14 for intc\n",
      "finished 2019-01-21 for intc\n",
      "finished 2019-01-28 for intc\n",
      "finished 2019-02-04 for intc\n",
      "finished 2019-02-11 for intc\n",
      "finished 2019-02-18 for intc\n",
      "finished 2019-02-25 for intc\n",
      "finished 2019-03-04 for intc\n",
      "finished 2019-03-11 for intc\n",
      "finished 2019-03-18 for intc\n",
      "finished 2019-03-25 for intc\n",
      "finished 2019-04-01 for intc\n",
      "finished 2019-04-08 for intc\n",
      "finished 2019-04-15 for intc\n",
      "finished 2019-04-22 for intc\n",
      "finished 2019-04-29 for intc\n",
      "finished 2019-05-06 for intc\n",
      "finished 2019-05-13 for intc\n",
      "finished 2019-05-20 for intc\n",
      "finished 2019-05-27 for intc\n",
      "finished 2019-06-03 for intc\n",
      "finished 2019-06-10 for intc\n",
      "finished 2019-06-17 for intc\n",
      "finished 2019-06-24 for intc\n",
      "************FINISHED WITH intc************\n",
      "finished 2018-12-24 for pypl\n",
      "finished 2018-12-31 for pypl\n",
      "finished 2019-01-07 for pypl\n",
      "finished 2019-01-14 for pypl\n",
      "finished 2019-01-21 for pypl\n",
      "finished 2019-01-28 for pypl\n",
      "finished 2019-02-04 for pypl\n",
      "finished 2019-02-11 for pypl\n",
      "finished 2019-02-18 for pypl\n",
      "finished 2019-02-25 for pypl\n",
      "finished 2019-03-04 for pypl\n",
      "finished 2019-03-11 for pypl\n",
      "finished 2019-03-18 for pypl\n",
      "finished 2019-03-25 for pypl\n",
      "finished 2019-04-01 for pypl\n",
      "finished 2019-04-08 for pypl\n",
      "finished 2019-04-15 for pypl\n",
      "finished 2019-04-22 for pypl\n",
      "finished 2019-04-29 for pypl\n",
      "finished 2019-05-06 for pypl\n",
      "finished 2019-05-13 for pypl\n",
      "finished 2019-05-20 for pypl\n",
      "finished 2019-05-27 for pypl\n",
      "finished 2019-06-03 for pypl\n",
      "finished 2019-06-10 for pypl\n",
      "finished 2019-06-17 for pypl\n",
      "finished 2019-06-24 for pypl\n",
      "************FINISHED WITH pypl************\n",
      "finished 2018-12-24 for snps\n",
      "finished 2018-12-31 for snps\n",
      "finished 2019-01-07 for snps\n",
      "finished 2019-01-14 for snps\n",
      "finished 2019-01-21 for snps\n",
      "finished 2019-01-28 for snps\n",
      "finished 2019-02-04 for snps\n",
      "finished 2019-02-11 for snps\n",
      "finished 2019-02-18 for snps\n",
      "finished 2019-02-25 for snps\n",
      "finished 2019-03-04 for snps\n",
      "finished 2019-03-11 for snps\n",
      "finished 2019-03-18 for snps\n",
      "finished 2019-03-25 for snps\n",
      "finished 2019-04-01 for snps\n",
      "finished 2019-04-08 for snps\n",
      "finished 2019-04-15 for snps\n",
      "finished 2019-04-22 for snps\n",
      "finished 2019-04-29 for snps\n",
      "finished 2019-05-06 for snps\n",
      "finished 2019-05-13 for snps\n",
      "finished 2019-05-20 for snps\n",
      "finished 2019-05-27 for snps\n",
      "finished 2019-06-03 for snps\n",
      "finished 2019-06-10 for snps\n",
      "finished 2019-06-17 for snps\n",
      "finished 2019-06-24 for snps\n",
      "************FINISHED WITH snps************\n",
      "finished 2018-12-24 for tmus\n",
      "finished 2018-12-31 for tmus\n",
      "finished 2019-01-07 for tmus\n",
      "finished 2019-01-14 for tmus\n",
      "finished 2019-01-21 for tmus\n",
      "finished 2019-01-28 for tmus\n",
      "finished 2019-02-04 for tmus\n",
      "finished 2019-02-11 for tmus\n",
      "finished 2019-02-18 for tmus\n",
      "finished 2019-02-25 for tmus\n",
      "finished 2019-03-04 for tmus\n",
      "finished 2019-03-11 for tmus\n",
      "finished 2019-03-18 for tmus\n",
      "finished 2019-03-25 for tmus\n",
      "finished 2019-04-01 for tmus\n",
      "finished 2019-04-08 for tmus\n",
      "finished 2019-04-15 for tmus\n",
      "finished 2019-04-22 for tmus\n",
      "finished 2019-04-29 for tmus\n",
      "finished 2019-05-06 for tmus\n",
      "finished 2019-05-13 for tmus\n",
      "finished 2019-05-20 for tmus\n",
      "finished 2019-05-27 for tmus\n",
      "finished 2019-06-03 for tmus\n",
      "finished 2019-06-10 for tmus\n",
      "finished 2019-06-17 for tmus\n",
      "finished 2019-06-24 for tmus\n",
      "************FINISHED WITH tmus************\n",
      "finished 2018-12-24 for trip\n",
      "finished 2018-12-31 for trip\n",
      "finished 2019-01-07 for trip\n",
      "finished 2019-01-14 for trip\n",
      "finished 2019-01-21 for trip\n",
      "finished 2019-01-28 for trip\n",
      "finished 2019-02-04 for trip\n",
      "finished 2019-02-11 for trip\n",
      "finished 2019-02-18 for trip\n",
      "finished 2019-02-25 for trip\n",
      "finished 2019-03-04 for trip\n",
      "finished 2019-03-11 for trip\n",
      "finished 2019-03-18 for trip\n",
      "finished 2019-03-25 for trip\n",
      "finished 2019-04-01 for trip\n",
      "finished 2019-04-08 for trip\n",
      "finished 2019-04-15 for trip\n",
      "finished 2019-04-22 for trip\n",
      "finished 2019-04-29 for trip\n",
      "finished 2019-05-06 for trip\n",
      "finished 2019-05-13 for trip\n",
      "finished 2019-05-20 for trip\n",
      "finished 2019-05-27 for trip\n",
      "finished 2019-06-03 for trip\n",
      "finished 2019-06-10 for trip\n",
      "finished 2019-06-17 for trip\n",
      "finished 2019-06-24 for trip\n",
      "************FINISHED WITH trip************\n",
      "finished 2018-12-24 for dis\n",
      "finished 2018-12-31 for dis\n",
      "finished 2019-01-07 for dis\n",
      "finished 2019-01-14 for dis\n",
      "finished 2019-01-21 for dis\n",
      "finished 2019-01-28 for dis\n",
      "finished 2019-02-04 for dis\n",
      "finished 2019-02-11 for dis\n",
      "finished 2019-02-18 for dis\n",
      "finished 2019-02-25 for dis\n",
      "finished 2019-03-04 for dis\n",
      "finished 2019-03-11 for dis\n",
      "finished 2019-03-18 for dis\n",
      "finished 2019-03-25 for dis\n",
      "finished 2019-04-01 for dis\n",
      "finished 2019-04-08 for dis\n",
      "finished 2019-04-15 for dis\n",
      "finished 2019-04-22 for dis\n",
      "finished 2019-04-29 for dis\n",
      "finished 2019-05-06 for dis\n",
      "finished 2019-05-13 for dis\n",
      "finished 2019-05-20 for dis\n",
      "finished 2019-05-27 for dis\n",
      "finished 2019-06-03 for dis\n",
      "finished 2019-06-10 for dis\n",
      "finished 2019-06-17 for dis\n",
      "finished 2019-06-24 for dis\n",
      "************FINISHED WITH dis************\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    df = multi_row_pipeline(dates, ticker)\n",
    "    master_df_set.append(df)\n",
    "    print(\"************FINISHED WITH {0}************\".format(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_test</th>\n",
       "      <th>indico_sentiment</th>\n",
       "      <th>bad_bag</th>\n",
       "      <th>good_bag</th>\n",
       "      <th>lastweek</th>\n",
       "      <th>headlines</th>\n",
       "      <th>absolute_change</th>\n",
       "      <th>macro_direction</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>0.161692</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>166.07</td>\n",
       "      <td>[Pilgrimage to see Warren Buffett out of step ...</td>\n",
       "      <td>-0.00294</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>0.161692</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>157.17</td>\n",
       "      <td>[Pilgrimage to see Warren Buffett out of step ...</td>\n",
       "      <td>0.00844</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>0.161692</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>157.92</td>\n",
       "      <td>[Pilgrimage to see Warren Buffett out of step ...</td>\n",
       "      <td>-0.00107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>2019-01-20</td>\n",
       "      <td>0.161692</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>150.75</td>\n",
       "      <td>[Pilgrimage to see Warren Buffett out of step ...</td>\n",
       "      <td>0.00219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>2019-01-27</td>\n",
       "      <td>0.161692</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>153.07</td>\n",
       "      <td>[Pilgrimage to see Warren Buffett out of step ...</td>\n",
       "      <td>-0.01050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Start         End  sentiment  sentiment_test  indico_sentiment  \\\n",
       "0  2018-12-24  2018-12-30   0.161692        0.113839           0.82943   \n",
       "0  2018-12-31  2019-01-06   0.161692        0.113839           0.82943   \n",
       "0  2019-01-07  2019-01-13   0.161692        0.113839           0.82943   \n",
       "0  2019-01-14  2019-01-20   0.161692        0.113839           0.82943   \n",
       "0  2019-01-21  2019-01-27   0.161692        0.113839           0.82943   \n",
       "\n",
       "    bad_bag  good_bag  lastweek  \\\n",
       "0  0.000004  0.000038    166.07   \n",
       "0  0.000004  0.000038    157.17   \n",
       "0  0.000004  0.000038    157.92   \n",
       "0  0.000004  0.000038    150.75   \n",
       "0  0.000004  0.000038    153.07   \n",
       "\n",
       "                                           headlines  absolute_change  \\\n",
       "0  [Pilgrimage to see Warren Buffett out of step ...         -0.00294   \n",
       "0  [Pilgrimage to see Warren Buffett out of step ...          0.00844   \n",
       "0  [Pilgrimage to see Warren Buffett out of step ...         -0.00107   \n",
       "0  [Pilgrimage to see Warren Buffett out of step ...          0.00219   \n",
       "0  [Pilgrimage to see Warren Buffett out of step ...         -0.01050   \n",
       "\n",
       "   macro_direction  delta  \n",
       "0                0      1  \n",
       "0                1      0  \n",
       "0                0      1  \n",
       "0                1      1  \n",
       "0                0      1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_mammoth_set = pd.concat(master_df_seter_df_setter_df_set)\n",
    "training_mammoth_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mammoth_set.to_csv(\"data/576_rows.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing to perform regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_mammoth_set[[\"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\", \"absolute_change\", \"macro_direction\"]]\n",
    "Y = training_mammoth_set[\"delta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest [no hyperparameter tuning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial random forest was constructed with 68.51851851851852% accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, Y_train)\n",
    "Y_predicted = rfc.predict(X_test)\n",
    "error = accuracy_score(Y_test, Y_predicted) * 100\n",
    "print(\"initial random forest was constructed with {0}% accuracy\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest [with tuned hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rfc, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, \n",
    "                               verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 90, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new and improved RandomForestClassifier\n",
    "rfc_tuned = RandomForestClassifier(n_estimators=400,\n",
    "                                  min_samples_split=10,\n",
    "                                  min_samples_leaf=2,\n",
    "                                  max_features=\"sqrt\",\n",
    "                                  max_depth=90,\n",
    "                                  bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned random forest was constructed with 70.98765432098766% accuracy\n"
     ]
    }
   ],
   "source": [
    "rfc_tuned.fit(X_train, Y_train)\n",
    "Y_predicted = rfc_tuned.predict(X_test)\n",
    "error = accuracy_score(Y_test, Y_predicted) * 100\n",
    "print(\"tuned random forest was constructed with {0}% accuracy\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial logistic regression was constructed with 66.0493827160494% accuracy\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver=\"lbfgs\")\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_predicted = logreg.predict(X_test)\n",
    "error = accuracy_score(Y_test, Y_predicted) * 100\n",
    "print(\"initial logistic regression was constructed with {0}% accuracy\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gaussian naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial gnb classifier was constructed with 63.580246913580254% accuracy\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "Y_predicted = gnb.predict(X_test)\n",
    "error = accuracy_score(Y_test, Y_predicted) * 100\n",
    "print(\"initial gnb classifier was constructed with {0}% accuracy\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickling the final random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=90, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=400,\n",
    "                                  min_samples_split=10,\n",
    "                                  min_samples_leaf=2,\n",
    "                                  max_features=\"sqrt\",\n",
    "                                  max_depth=90,\n",
    "                                  bootstrap=False)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'data/logistic.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
