{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "import indicoio\n",
    "import seaborn as sns\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = []\n",
    "DOWN = []\n",
    "\n",
    "def count_words(original_string, bag_of_words):\n",
    "    split_string = original_string.lower().split()\n",
    "    count = 0\n",
    "    for word in bag_of_words:\n",
    "        matches = split_string.count(word)\n",
    "        count += matches\n",
    "    ratio = count/len(split_string)\n",
    "    return ratio\n",
    "    \n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "    \n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "    \n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(np.mean)\n",
    "    df[\"rick_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "    # add statistical features\n",
    "    # df[\"features\"] = df[\"corpus\"].apply(lambda text: indicoio.text_features(text, v=2))\n",
    "    \n",
    "    # add bag of words ratio\n",
    "    #df[\"bad_ratio\"] = \n",
    "    #df[\"good_keywords\"]\n",
    "    \n",
    "    # return the nice beautiful dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(d, ticky):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    d[\"constvec\"] = [0.7] * len(d)\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    df.lastweek = df.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    X = d[[\"Open\", \"lastweek\", \"rick_sentiment\"]]\n",
    "    Y = d[\"Close\"]\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    def mse(v1, v2):\n",
    "        return np.sum((v1 - v2) ** 2) \n",
    "    \n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = mse(Y_predicted, Y_test)\n",
    "    print(\"aggregated error : {0}\".format(error))\n",
    "    print(\"average error by prediction for {1}: {0}\".format(np.mean(Y_predicted - Y_test), ticky))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 30.467437028884888s for aapl\n",
      "aggregated error : 158.50868881699554\n",
      "average error by prediction for aapl: 0.08792566878034967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('aapl-6m-weekly.csv', 'aapl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'aapl'))\n",
    "predictor(df, 'aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 9.660725593566895s for crm\n",
      "aggregated error : 107.38549891871838\n",
      "average error by prediction for crm: 2.22604476549286\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('crm-6m-weekly.csv', 'crm')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'crm'))\n",
    "predictor(df, 'crm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 12.82651162147522s for tsla\n",
      "aggregated error : 2973.1671609501464\n",
      "average error by prediction for tsla: -11.812941524549151\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('tsla-6m-weekly.csv', 'tsla')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'tsla'))\n",
    "predictor(df, 'tsla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 10.964613437652588s for fb\n",
      "aggregated error : 156.56163627135112\n",
      "average error by prediction for fb: 1.2611638446461138\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('fb-6m-weekly.csv', 'fb')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'fb'))\n",
    "predictor(df, 'fb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 12.713487386703491s for baba\n",
      "aggregated error : 346.6137871138725\n",
      "average error by prediction for baba: -3.175124251872483\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('baba-6m-weekly.csv', 'baba')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'baba'))\n",
    "predictor(df, 'baba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 11.158455848693848s for msft\n",
      "aggregated error : 48.333944998318856\n",
      "average error by prediction for msft: 0.8648414383188638\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('msft-6m-weekly.csv', 'msft')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'msft'))\n",
    "predictor(df, 'msft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 12.216991424560547s for nflx\n",
      "aggregated error : 1154.4152604516844\n",
      "average error by prediction for nflx: -6.532616521740055\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('nflx-6m-weekly.csv', 'nflx')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'nflx'))\n",
    "predictor(df, 'nflx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 11.339673519134521s for amzn\n",
      "aggregated error : 3462.678062502195\n",
      "average error by prediction for amzn: 4.462062612569172\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('amzn-6m-weekly.csv', 'amzn')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'amzn'))\n",
    "predictor(df, 'amzn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 13.014968395233154s for googl\n",
      "aggregated error : 16801.007336800532\n",
      "average error by prediction for googl: 26.4038610113791\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('googl-6m-weekly.csv', 'googl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'googl'))\n",
    "predictor(df, 'googl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 13.363798379898071s for wmt\n",
      "aggregated error : 39.68606497767715\n",
      "average error by prediction for wmt: 0.22019270404814031\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('wmt-6m-weekly.csv', 'wmt')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'wmt'))\n",
    "predictor(df, 'wmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
