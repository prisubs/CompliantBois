{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "import indicoio\n",
    "import seaborn as sns\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(string_vector):\n",
    "    bad_bag = ...\n",
    "    good_bag = ...\n",
    "    bad_count, good_count = 0\n",
    "    for word in bad_bag:\n",
    "        bad_count += string_vector.count(word)\n",
    "    for word in good_bag:\n",
    "        good_count += string_vector.count(word)\n",
    "    bad = bad_count/len(string_vector)\n",
    "    good = good_count/len(string_vector)\n",
    "    return good, bad\n",
    "    \n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "    \n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "    \n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "    \n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(np.mean)\n",
    "    df[\"indico_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "    \n",
    "    # add bag of words featurization\n",
    "    df[\"bad_bag\"], df[\"good_bag\"] = df[\"vectorized\"].apply(bag_of_words)\n",
    "    # add statistical features\n",
    "    # df[\"features\"] = df[\"corpus\"].apply(lambda text: indicoio.text_features(text, v=2))\n",
    "    \n",
    "    # add bag of words ratio\n",
    "    #df[\"bad_ratio\"] = \n",
    "    #df[\"good_keywords\"]\n",
    "    \n",
    "    # return the nice beautiful dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(d, ticky):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    d[\"constvec\"] = [0.7] * len(d)\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    df.lastweek = df.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    X = d[[\"Open\", \"lastweek\", \"indico_sentiment\"]]\n",
    "    Y = d[\"Close\"]\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    def mse(v1, v2):\n",
    "        return np.sum((v1 - v2) ** 2) \n",
    "    \n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = mse(Y_predicted, Y_test)\n",
    "    print(\"aggregated error : {0}\".format(error))\n",
    "    print(\"average error by prediction for {1}: {0}\".format(np.mean(Y_predicted - Y_test), ticky))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 31.345920085906982s for aapl\n",
      "aggregated error : 158.50476680221217\n",
      "average error by prediction for aapl: 0.0879561589843484\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('aapl-6m-weekly.csv', 'aapl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'aapl'))\n",
    "predictor(df, 'aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 28.644730806350708s for crm\n",
      "aggregated error : 87.24971769126635\n",
      "average error by prediction for crm: 1.8844314922405028\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('crm-6m-weekly.csv', 'crm')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'crm'))\n",
    "predictor(df, 'crm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 36.25160598754883s for tsla\n",
      "aggregated error : 2980.1651149210297\n",
      "average error by prediction for tsla: -11.743885306369817\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('tsla-6m-weekly.csv', 'tsla')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'tsla'))\n",
    "predictor(df, 'tsla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 30.20443606376648s for fb\n",
      "aggregated error : 112.77294440289208\n",
      "average error by prediction for fb: 0.1187069821744381\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('fb-6m-weekly.csv', 'fb')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'fb'))\n",
    "predictor(df, 'fb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 36.11968111991882s for baba\n",
      "aggregated error : 343.35289975565377\n",
      "average error by prediction for baba: -3.3095244772874413\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('baba-6m-weekly.csv', 'baba')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'baba'))\n",
    "predictor(df, 'baba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 38.20546102523804s for msft\n",
      "aggregated error : 44.333149430340875\n",
      "average error by prediction for msft: 0.7658176359434081\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('msft-6m-weekly.csv', 'msft')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'msft'))\n",
    "predictor(df, 'msft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 32.990135192871094s for nflx\n",
      "aggregated error : 1579.151981114193\n",
      "average error by prediction for nflx: -8.735665936461146\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('nflx-6m-weekly.csv', 'nflx')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'nflx'))\n",
    "predictor(df, 'nflx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 31.7349591255188s for amzn\n",
      "aggregated error : 3559.7040908256663\n",
      "average error by prediction for amzn: 9.34027130103315\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('amzn-6m-weekly.csv', 'amzn')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'amzn'))\n",
    "predictor(df, 'amzn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 34.03275227546692s for googl\n",
      "aggregated error : 14629.308640535517\n",
      "average error by prediction for googl: 27.57321798135427\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('googl-6m-weekly.csv', 'googl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'googl'))\n",
    "predictor(df, 'googl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 34.973336935043335s for wmt\n",
      "aggregated error : 55.831140343965274\n",
      "average error by prediction for wmt: -0.594398543408758\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('wmt-6m-weekly.csv', 'wmt')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'wmt'))\n",
    "predictor(df, 'wmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outperform',\n",
       " 'voided',\n",
       " 'discrepancy',\n",
       " 'arbitrary',\n",
       " 'imbalance',\n",
       " 'overcharged',\n",
       " 'manipulation',\n",
       " 'distressed',\n",
       " 'hazard',\n",
       " 'unfit',\n",
       " 'defraud',\n",
       " 'undefined',\n",
       " 'malpractice',\n",
       " 'presumes',\n",
       " 'collapsing',\n",
       " 'falsely',\n",
       " 'unsound',\n",
       " 'damaging',\n",
       " 'reassignment',\n",
       " 'distracting',\n",
       " 'disapproved',\n",
       " 'stagnant',\n",
       " 'unsafe',\n",
       " 'critically',\n",
       " 'duress',\n",
       " 'pleadings',\n",
       " 'investigated',\n",
       " 'sometime',\n",
       " 'encroachment',\n",
       " 'forfeits',\n",
       " 'rejections',\n",
       " 'whereabouts',\n",
       " 'unmatched',\n",
       " 'confident',\n",
       " 'rewarded',\n",
       " 'prosperity',\n",
       " 'rectification',\n",
       " 'forfeitable',\n",
       " 'turmoil',\n",
       " 'progresses',\n",
       " 'antecedent',\n",
       " 'dissolutions',\n",
       " 'expropriation',\n",
       " 'understate',\n",
       " 'misstate',\n",
       " 'mutandis',\n",
       " 'delisting',\n",
       " 'uncovers',\n",
       " 'grantors',\n",
       " 'predeceases']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sandbox for turning bag of words into a feature\n",
    "bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "good = bag[\"Good Words\"].apply(str.lower).to_list()\n",
    "bad = bag[\"Bad Words\"].apply(str.lower).to_list()\n",
    "good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
