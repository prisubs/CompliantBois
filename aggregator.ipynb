{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "import indicoio\n",
    "import seaborn as sns\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = []\n",
    "DOWN = []\n",
    "\n",
    "def count_words(original_string, bag_of_words):\n",
    "    split_string = original_string.lower().split()\n",
    "    count = 0\n",
    "    for word in bag_of_words:\n",
    "        matches = split_string.count(word)\n",
    "        count += matches\n",
    "    ratio = count/len(split_string)\n",
    "    return ratio\n",
    "    \n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "    \n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "    \n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(np.mean)\n",
    "    df[\"indico_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "    # add statistical features\n",
    "    # df[\"features\"] = df[\"corpus\"].apply(lambda text: indicoio.text_features(text, v=2))\n",
    "    \n",
    "    # add bag of words ratio\n",
    "    #df[\"bad_ratio\"] = \n",
    "    #df[\"good_keywords\"]\n",
    "    \n",
    "    # return the nice beautiful dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(d, ticky):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    d[\"constvec\"] = [0.7] * len(d)\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    df.lastweek = df.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    X = d[[\"Open\", \"lastweek\", \"indico_sentiment\"]]\n",
    "    Y = d[\"Close\"]\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    def mse(v1, v2):\n",
    "        return np.sum((v1 - v2) ** 2) \n",
    "    \n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = mse(Y_predicted, Y_test)\n",
    "    print(\"aggregated error : {0}\".format(error))\n",
    "    print(\"average error by prediction for {1}: {0}\".format(np.mean(Y_predicted - Y_test), ticky))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 34.08493709564209s for aapl\n",
      "aggregated error : 158.5053123445582\n",
      "average error by prediction for aapl: 0.08795332615945187\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('aapl-6m-weekly.csv', 'aapl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'aapl'))\n",
    "predictor(df, 'aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 30.264259099960327s for crm\n",
      "aggregated error : 87.2436031751756\n",
      "average error by prediction for crm: 1.8843514114534072\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('crm-6m-weekly.csv', 'crm')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'crm'))\n",
    "predictor(df, 'crm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 33.41849899291992s for tsla\n",
      "aggregated error : 2980.1666694736023\n",
      "average error by prediction for tsla: -11.74392175573135\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('tsla-6m-weekly.csv', 'tsla')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'tsla'))\n",
    "predictor(df, 'tsla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 30.157266855239868s for fb\n",
      "aggregated error : 112.77374401165784\n",
      "average error by prediction for fb: 0.11871369705476001\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('fb-6m-weekly.csv', 'fb')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'fb'))\n",
    "predictor(df, 'fb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 34.08108711242676s for baba\n",
      "aggregated error : 343.35529935324485\n",
      "average error by prediction for baba: -3.3095867717986556\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('baba-6m-weekly.csv', 'baba')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'baba'))\n",
    "predictor(df, 'baba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 34.297032833099365s for msft\n",
      "aggregated error : 44.32900935243161\n",
      "average error by prediction for msft: 0.765565859534724\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('msft-6m-weekly.csv', 'msft')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'msft'))\n",
    "predictor(df, 'msft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 32.15219712257385s for nflx\n",
      "aggregated error : 1579.1503591096744\n",
      "average error by prediction for nflx: -8.735652954215588\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('nflx-6m-weekly.csv', 'nflx')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'nflx'))\n",
    "predictor(df, 'nflx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 31.631071090698242s for amzn\n",
      "aggregated error : 3559.698342212954\n",
      "average error by prediction for amzn: 9.340305573201501\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('amzn-6m-weekly.csv', 'amzn')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'amzn'))\n",
    "predictor(df, 'amzn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 36.64185070991516s for googl\n",
      "aggregated error : 14629.32763112144\n",
      "average error by prediction for googl: 27.57323368072121\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('googl-6m-weekly.csv', 'googl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'googl'))\n",
    "predictor(df, 'googl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 34.182809829711914s for wmt\n",
      "aggregated error : 55.831026301367956\n",
      "average error by prediction for wmt: -0.5945596814515078\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('wmt-6m-weekly.csv', 'wmt')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'wmt'))\n",
    "predictor(df, 'wmt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
