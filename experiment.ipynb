{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import datasets\\n\\n# import some data to play with\\niris = datasets.load_iris()\\nX = iris.data[:, :2]  # we only take the first two features.\\nprint(X)\\nY = iris.target\\nprint(Y)\\n\\nlogreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\\n\\n# Create an instance of Logistic Regression Classifier and fit the data.\\nlogreg.fit(X, Y)\\n\\n# Plot the decision boundary. For that, we will assign a color to each\\n# point in the mesh [x_min, x_max]x[y_min, y_max].\\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\\nh = .02  # step size in the mesh\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\\n\\n# Put the result into a color plot\\nZ = Z.reshape(xx.shape)\\nplt.figure(1, figsize=(4, 3))\\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\\n\\n# Plot also the training points\\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\\nplt.xlabel('Sepal length')\\nplt.ylabel('Sepal width')\\n\\nplt.xlim(xx.min(), xx.max())\\nplt.ylim(yy.min(), yy.max())\\nplt.xticks(())\\nplt.yticks(())\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(__doc__)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Code source: GaÃ«l Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "print(X)\n",
    "Y = iris.target\n",
    "print(Y)\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = []\n",
    "DOWN = []\n",
    "\n",
    "def count_words(original_string, bag_of_words):\n",
    "    split_string = original_string.lower().split()\n",
    "    count = 0\n",
    "    for word in bag_of_words:\n",
    "        matches = split_string.count(word)\n",
    "        count += matches\n",
    "    ratio = count/len(split_string)\n",
    "    return ratio\n",
    "    \n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "    \n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "    \n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(np.mean)\n",
    "    #df[\"indico_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "    # add statistical features\n",
    "    # df[\"features\"] = df[\"corpus\"].apply(lambda text: indicoio.text_features(text, v=2))\n",
    "    \n",
    "    # add bag of words ratio\n",
    "    #df[\"bad_ratio\"] = \n",
    "    #df[\"good_keywords\"]\n",
    "    \n",
    "    # return the nice beautiful dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(d, ticky):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    d[\"constvec\"] = [0.7] * len(d)\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    df.lastweek = df.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    X = d[[\"Open\", \"lastweek\", \"indico_sentiment\"]]\n",
    "    Y = d[\"Close\"]\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    def mse(v1, v2):\n",
    "        return np.sum((v1 - v2) ** 2) \n",
    "    \n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = mse(Y_predicted, Y_test)\n",
    "    print(\"aggregated error : {0}\".format(error))\n",
    "    print(\"average error by prediction for {1}: {0}\".format(np.mean(Y_predicted - Y_test), ticky))\n",
    "    \n",
    "    \n",
    "    \n",
    "def predictor_two(d, ticky):\n",
    "    import matplotlib.pyplot as plot\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn import utils\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    d.lastweek = d.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    #X = np.c_[np.array(d[\"Open\"]), np.array(d[\"indico_sentiment\"])]\n",
    "    print(d.head())\n",
    "    X = np.vstack((d[\"Open\"],  d[\"sentiment\"])).T\n",
    "    Y = d[\"Adj Close\"]\n",
    "    Y = (Y - d[\"lastweek\"]) > 0\n",
    "    # import some data to play with\n",
    "    print(X)\n",
    "    print(\"X length:\")\n",
    "    print(X.size)\n",
    "    print(Y)\n",
    "    print(\"Y length:\")\n",
    "    print(Y.size)\n",
    "    logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "    # Create an instance of Logistic Regression Classifier and fit the data.\n",
    "    logreg.fit(X, Y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.xlabel('OPEN')\n",
    "    plt.ylabel('SENTIMENT')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline took 31.01529049873352s for aapl\n",
      "aggregated error : 158.50752155650392\n",
      "average error by prediction for aapl: 0.08794019001604486\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pipeline('aapl-6m-weekly.csv', 'aapl')\n",
    "end = time.time()\n",
    "pipeline_time = end - start\n",
    "print(\"pipeline took {0}s for {1}\".format(pipeline_time, 'aapl'))\n",
    "predictor(df, 'aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline('aapl-6m-weekly.csv', 'aapl')\n",
    "predictor_two(df, 'aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
