{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression bois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tiingo import TiingoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "import indicoio\n",
    "import seaborn as sns\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature vector creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "    good_bag = bag[\"Good Words\"].apply(str.lower).to_list()\n",
    "    good_count = 0\n",
    "    for word in good_bag:\n",
    "        good_count += string_vector.count(word)\n",
    "    good = good_count/len(string_vector)\n",
    "    return good\n",
    "\n",
    "def bad_bag(string_vector):\n",
    "    bag = pd.read_csv(\"data/bag_of_words.csv\")\n",
    "    bad_bag = bag[\"Bad Words\"].apply(str.lower).to_list()\n",
    "    bad_count = 0\n",
    "    for word in bad_bag:\n",
    "        bad_count += string_vector.count(word)\n",
    "    bad = bad_count/len(string_vector)\n",
    "    return bad\n",
    "    \n",
    "def aggregate_jsons(json_list):\n",
    "    result = []\n",
    "    for json in json_list:\n",
    "        headline = json[\"title\"]\n",
    "        desc = json[\"description\"]\n",
    "        x = headline + \" \" + desc\n",
    "        result.append(x)\n",
    "    return result\n",
    "\n",
    "def pipeline(df_path, ticky):\n",
    "    # get the dataframe from csv\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # fix dates\n",
    "    df[\"Date\"] = df[\"Start\"].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True))\n",
    "    df[\"Start\"] = df[\"Date\"].apply(str)\n",
    "    six_days = lambda start_date: start_date + datetime.timedelta(days=6)\n",
    "    df[\"End\"] = df[\"Date\"].apply(six_days).apply(str)\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    remove_time = lambda dt: dt[0:10]\n",
    "    df[\"Start\"] = df[\"Start\"].apply(remove_time)\n",
    "    df[\"End\"] = df[\"End\"].apply(remove_time)\n",
    "    \n",
    "    # add json\n",
    "    client = TiingoClient({\"api_key\": \"a265fc4a1013923f970d16e7348195074e97fcb0\"})\n",
    "    query_ticker = lambda t, s, e: client.get_news(tickers=[t], startDate=s, endDate=e)\n",
    "    df[\"JSON\"] = df.apply(lambda d: query_ticker(ticky, d[\"Start\"], d[\"End\"]), axis=1)\n",
    "    \n",
    "    # create corpus\n",
    "    df[\"corpus\"] = df[\"JSON\"].apply(aggregate_jsons)\n",
    "    df[\"vectorized\"] = df[\"corpus\"]\n",
    "    combinatric = lambda l: ''.join(l)\n",
    "    df[\"corpus\"] = df[\"vectorized\"].apply(combinatric)\n",
    "    indicoio.config.api_key = \"25b83c4c388204edd2c6c11cd907e048\"\n",
    "    \n",
    "    # add sentiment\n",
    "    df[\"sentiment\"] = df[\"corpus\"].apply(lambda orig: TextBlob(orig).sentiment.polarity)\n",
    "    df[\"sentiment_test\"] = df[\"vectorized\"].apply(lambda orig: [TextBlob(o).sentiment.polarity for o in orig]).apply(np.mean)\n",
    "    df[\"indico_sentiment\"] = df[\"corpus\"].apply(lambda text: indicoio.sentiment_hq(text))\n",
    "    \n",
    "    # add bag of words featurization\n",
    "    df[\"bad_bag\"] = df[\"vectorized\"].apply(bad_bag)\n",
    "    df[\"good_bag\"] = df[\"vectorized\"].apply(good_bag)\n",
    "    \n",
    "    # return the nice beautiful dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bayesian ridge regression\n",
    "predicting close price from open and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(v1, v2):\n",
    "        return np.sum((v1 - v2) ** 2) \n",
    "\n",
    "def impute(X):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer(d, ticky):\n",
    "    d[\"lastweek\"] = d[\"Close\"]\n",
    "    d.lastweek = d.lastweek.shift(7) ## shift down\n",
    "    #df.lastweek.drop(df.gdp.shape[0] - 1,inplace = True) ## removing the first row\n",
    "    X = d[[\"Open\", \"lastweek\", \"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\"]]\n",
    "    Y = d[\"Close\"]\n",
    "    \n",
    "    X = impute(X)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 100)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "    \n",
    "    model = linear_model.BayesianRidge()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    error = mse(Y_predicted, Y_test)\n",
    "    print(\"aggregated error : {0}\".format(error))\n",
    "    print(\"average error by prediction for {1}: {0}\".format(np.mean(Y_predicted - Y_test), ticky))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ticker_linear(ticky):\n",
    "    print(\"******{0}*********\".format(ticky)*5)\n",
    "    start = time.time()\n",
    "    df = pipeline('6m-weekly/{0}-6m-weekly.csv'.format(ticky), ticky)\n",
    "    end = time.time()\n",
    "    pipeline_time = end - start\n",
    "    print(\"pipeline took {0}s for {1}\".format(pipeline_time, ticky))\n",
    "    imputer(df, ticky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['aapl', 'crm', 'tsla', 'fb', 'baba', 'msft', 'nflx', 'amzn', 'googl', 'wmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******aapl***************aapl***************aapl***************aapl***************aapl*********\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/../data/6m-weekly/aapl-6m-weekly.csv' does not exist: b'/../data/6m-weekly/aapl-6m-weekly.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52d3920fae34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_ticker_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process took {} minutes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9c42e956441e>\u001b[0m in \u001b[0;36mrun_ticker_linear\u001b[0;34m(ticky)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"******{0}*********\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticky\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/../data/6m-weekly/{0}-6m-weekly.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticky\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticky\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpipeline_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c7d868a465f8>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(df_path, ticky)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticky\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# get the dataframe from csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# fix dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/../data/6m-weekly/aapl-6m-weekly.csv' does not exist: b'/../data/6m-weekly/aapl-6m-weekly.csv'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for ticker in tickers:\n",
    "    run_ticker_linear(ticker)\n",
    "end = time.time()\n",
    "print(\"process took {} minutes\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression\n",
    "predicting whether it will be an up week or down week based on same features as above\n",
    "<br>\n",
    "lower accuracy so probably discarding this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the up/down label\n",
    "# DOWN - 0\n",
    "# UP - 1\n",
    "\n",
    "def logistic_accuracy(y_pred, y_real):\n",
    "    y_pred, y_real = list(y_pred), list(y_real)\n",
    "    ct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_real[i]:\n",
    "            ct += 1\n",
    "    return ct/len(y_pred)\n",
    "\n",
    "def add_updown(df):\n",
    "    difs = df[\"Open\"] - df[\"Close\"]\n",
    "    x = pd.Series([1 if dif < 0 else 0 for dif in difs])\n",
    "    df[\"triangle\"] = x\n",
    "    return df\n",
    "\n",
    "def prepare_logistic(path, ticky):\n",
    "    df = pipeline(path, ticky)\n",
    "    df = add_updown(df)\n",
    "    return df\n",
    "\n",
    "def logistic_reg(df, ticky):\n",
    "    X = impute(df[[\"indico_sentiment\", \"sentiment\", \"sentiment_test\", \"bad_bag\", \"good_bag\"]])\n",
    "    Y = df[\"triangle\"]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 100)\n",
    "    \n",
    "    model = linear_model.LogisticRegression(solver=\"lbfgs\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    \n",
    "    error_pct = logistic_accuracy(Y_predicted, Y_test)\n",
    "    return error_pct\n",
    "\n",
    "def run_logistic(ticker_list):\n",
    "    print(\"beginning logistic regression\")\n",
    "    for ticker in tickers:\n",
    "        print(\"******{0}*********\".format(ticker)*5)\n",
    "        filepath = 'data/6m-weekly/{0}-6m-weekly.csv'.format(ticker)\n",
    "        start = time.time()\n",
    "        df = prepare_logistic(filepath, ticker)\n",
    "        end = time.time()\n",
    "        pipeline_time = end - start\n",
    "        print(\"pipeline took {0}s for {1}\".format(pipeline_time, ticker))\n",
    "        accuracy_pct = logistic_reg(df, ticker) * 100\n",
    "        print(\"ticker: {0}\\naccuracy{1}%\".format(ticker, accuracy_pct))\n",
    "    print(\"finished logistic regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic(tickers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
